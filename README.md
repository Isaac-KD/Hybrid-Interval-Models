# üß† MacsumNet & HybridNet: Architectures Neuronales pour la R√©gression par Intervalles

Ce d√©p√¥t pr√©sente une nouvelle famille de mod√®les de deep learning pour la **r√©gression par intervalles**, une t√¢che visant √† pr√©dire un intervalle `[borne_inf√©rieure, borne_sup√©rieure]` plut√¥t qu'une valeur unique. Le point culminant de ce projet de recherche est **`HybridNet`**, une architecture flexible qui fusionne la puissance d'extraction de caract√©ristiques des perceptrons traditionnels avec un nouvel op√©rateur d'agr√©gation d√©pendant de l'ordre, appel√© **`Macsum`**.

Le r√©sultat est un mod√®le capable de quantifier son incertitude, une propri√©t√© essentielle pour des applications fiables en IA.

![Exemple de performance de HybridNet sur des donn√©es complexes](Experimentation/Hybride_L_input/16L_8M_4L_1.png)
*Exemple de pr√©diction d'intervalles par un mod√®le `HybridNet`. L'intervalle pr√©dit (zone ombr√©e) encadre avec succ√®s la v√©rit√© terrain (points).*

---

### Navigation
- [Motivation](#üéØ-motivation--aller-au-del√†-de-la-pr√©diction-ponctuelle)
- [L'Op√©rateur Macsum](#‚öôÔ∏è-lop√©rateur-macsum)
- [Architectures D√©velopp√©es](#üèóÔ∏è-architectures-d√©velopp√©es)
- [Structure du D√©p√¥t](#üìÅ-structure-du-d√©p√¥t)
- [Installation](#üöÄ-installation-et-d√©marrage)
- [Exemple d'Utilisation](#üß™-exemple-dutilisation)
- [R√©sultats](#üìä-r√©sultats-et-analyses)
- [Perspectives](#üî≠-perspectives-et-travaux-futurs)

---

## üéØ Motivation : Aller au-del√† de la Pr√©diction Ponctuelle

Les mod√®les de r√©gression classiques fournissent des pr√©dictions ponctuelles, occultant toute notion d'incertitude. Dans des domaines critiques comme la finance, la m√©t√©orologie ou la maintenance pr√©dictive, cette information est pourtant cruciale. La r√©gression par intervalles r√©pond √† ce besoin en encadrant la v√©rit√© terrain par un intervalle de confiance. Ce projet vise √† concevoir des r√©seaux de neurones profonds capables de produire nativement et efficacement de tels intervalles.

---

## üìÑ Rapport de Recherche

Une description d√©taill√©e de la motivation, des fondements th√©oriques, de l'architecture du mod√®le et de l'analyse des r√©sultats est disponible dans le rapport de recherche complet r√©dig√© pour ce projet.

‚û°Ô∏è **[Lire le rapport complet (PDF)](./rapport.pdf)**

---


## ‚öôÔ∏è L'Op√©rateur Macsum

Au c≈ìur de notre approche se trouve **l'op√©rateur Macsum**, un agr√©gateur non lin√©aire dont la particularit√© est d'√™tre bas√© sur le **tri** des caract√©ristiques d'entr√©e selon un vecteur de poids appris `œÜ`. Contrairement aux op√©rateurs classiques comme la somme ou la moyenne, Macsum exploite l'**ordre** des features pour mod√©liser des interactions complexes, ce qui le rend particuli√®rement apte √† construire des bornes de pr√©diction robustes.

---

## üèóÔ∏è Architectures D√©velopp√©es

### üîπ MacsumNet : Un R√©seau Homog√®ne

Une premi√®re approche a consist√© √† construire un r√©seau profond compos√© uniquement de couches `Macsum`. Bien que fonctionnelle, cette architecture a r√©v√©l√© des instabilit√©s lors de l'optimisation, soulignant la difficult√© d'entra√Æner des r√©seaux profonds bas√©s sur des non-lin√©arit√©s aussi exotiques.

### üîπ HybridNet : Le Meilleur des Deux Mondes

Le mod√®le phare de ce projet, `HybridNet`, a √©t√© con√ßu pour surmonter ces limitations en adoptant une approche synergique :
- **Couches `Linear` + Activation :** Agissent comme des extracteurs de caract√©ristiques universels, transformant les donn√©es brutes en un espace de repr√©sentation s√©mantiquement plus riche.
- **Couches `Macsum` + Activation :** Se sp√©cialisent dans l'agr√©gation de ces caract√©ristiques de haut niveau pour produire l'intervalle final.

Pour permettre cette h√©t√©rog√©n√©it√©, les couches `Linear` ont √©t√© √©tendues pour manipuler des intervalles en s'appuyant sur l'**arithm√©tique des intervalles** formalis√©e par Ramon E. Moore :
```math
y_{\text{lower}} = W^+ x_{\text{lower}} - W^- x_{\text{upper}} + b \\
y_{\text{upper}} = W^+ x_{\text{upper}} - W^- x_{\text{lower}} + b
```
o√π `W‚Å∫ = max(0, W)` et `W‚Åª = max(0, -W)`. Cette combinaison permet d'allier la stabilit√© des MLP √† la puissance de mod√©lisation de `Macsum`.

---

## üìÅ Structure du D√©p√¥t

```
.
‚îú‚îÄ‚îÄ Code/
‚îÇ   ‚îú‚îÄ‚îÄ Numpy/                  # Impl√©mentation initiale (legacy)
‚îÇ   ‚îî‚îÄ‚îÄ Torsh/                  # Impl√©mentation PyTorch finale
‚îÇ       ‚îú‚îÄ‚îÄ NetowrkMacsum.py   # D√©finition des architectures (HybridNet, etc.)
‚îÇ       ‚îú‚îÄ‚îÄ TorchMacsumAggregationLearning.py   # Classe de base Macsum et fonctions d'entra√Ænement
‚îÇ       ‚îî‚îÄ‚îÄ Torsh_data_generation.py            # Scripts pour g√©n√©rer des donn√©es synth√©tiques
‚îÇ
‚îú‚îÄ‚îÄ Experimentation/           # Collection de graphiques et r√©sultats d'exp√©riences
‚îú‚îÄ‚îÄ noteboock_test/            # Notebooks Jupyter pour les tests, l'exploration et la visualisation
‚îú‚îÄ‚îÄ rapport.tex                # Rapport de stage au format LaTeX
‚îî‚îÄ‚îÄ README.md                  # Ce fichier
```

---

## üöÄ Installation et D√©marrage

Ce projet a √©t√© d√©velopp√© et test√© avec **Python 3.9**. Il est recommand√© d'utiliser une version de Python >= 3.9 pour assurer la compatibilit√©.

Pour explorer le code et lancer vos propres exp√©riences, suivez ces √©tapes :

1.  **Clonez le d√©p√¥t :**
    ```bash
    git clone https://github.com/Isaac-KD/Macsum-aggregation-learning---Stage-LIP6--.git
    cd Macsum-aggregation-learning---Stage-LIP6--
    ```

2.  **Cr√©ez un environnement virtuel (recommand√©) :**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # Sur macOS/Linux
    # venv\Scripts\activate   # Sur Windows
    ```
3.  **Installez les d√©pendances via le fichier `requirements.txt` :**
    ```bash
    pip install -r requirements.txt
    ```
    **Installez les d√©pendances sans le fichier**
    ```bash
    pip install torch numpy pandas matplotlib scikit-learn tqdm plotly typing-extensions
    ```

---

## üß™ Exemple d'Utilisation

`HybridNet` est con√ßu pour √™tre simple √† utiliser. Voici un exemple complet pour entra√Æner un mod√®le :

```python
import torch.nn.functional as F
import numpy as np
from Code.Torsh.NetowrkMacsum import HybridNet
from Code.Torsh.TorchMacsumAggregationLearning import MacsumSigmoidTorch
from Code.Torsh.Torsh_data_generation import generate_friedman1_data

# 1. Pour la d√©monstration, cr√©ons des donn√©es factices
N_SAMPLES, N_FEATURES = 1000, 10
X, Y = generate_friedman1_data(N_SAMPLES, N_FEATURES, noise_std=0.0)
X_test, Y_test = generate_friedman1_data(500, N_FEATURES, noise_std=0.0)

# 2. D√©finissez une architecture profonde et h√©t√©rog√®ne
architecture = [
    {'type': 'linear', 'neurons': 128, 'activation': F.leaky_relu},
    {'type': 'linear', 'neurons': 64, 'activation': F.leaky_relu},
    {'type': 'macsum', 'neurons': 32, 'activation': F.leaky_relu},
    {'type': 'linear', 'neurons': 16, 'activation': F.leaky_relu},
    {'type': 'macsum', 'neurons': 1} # Couche de sortie
]

# 3. Initialisez le mod√®le avec ses hyperparam√®tres
model = HybridNet(
    input_dim=N_FEATURES,
    architecture=architecture,
    macsum_model_class=MacsumSigmoidTorch,
    alpha=0.1,      # Poids de la largeur de l'intervalle
    gamma=2.0,      # Poids de la p√©nalit√© de non-contenance
    k_sigmoid=0.1
)

# 4. Lancez l'entra√Ænement avec la m√©thode autograd
model.fit_autograd(
    X_train, Y_train,
    X_eval, Y_eval,
    learning_rate=1e-4,
    n_epochs=500,
    weight_decay=1e-5 # R√©gularisation pour √©viter le sur-apprentissage
)
```

---

## üìä R√©sultats et Analyses

Les architectures hybrides, en particulier les topologies profondes comme **`4L-2M-2L-1M`**, se sont montr√©es particuli√®rement performantes. Elles parviennent √† g√©n√©rer des intervalles qui sont √† la fois **pr√©cis** (faible largeur) et **fiables** (haut taux de couverture), m√™me sur des jeux de donn√©es complexes.

> ![Comparaison de performance](Experimentation/Hybride_L_input/4L_2_2L_1.png)

L'analyse des dynamiques d'entra√Ænement a r√©v√©l√© des ph√©nom√®nes int√©ressants, comme l‚Äô**effondrement de la borne inf√©rieure √† z√©ro**. Ce probl√®me a √©t√© diagnostiqu√© comme une "solution paresseuse" de l'optimiseur et corrig√© en ajustant les hyperparam√®tres de la fonction de co√ªt (notamment le ratio `alpha`/`gamma`) et en favorisant des architectures qui d√©butent par des couches `Linear` pour stabiliser l'extraction de caract√©ristiques.

---


## üî≠ Perspectives et Travaux Futurs

Ce framework est une fondation solide pour de nombreuses explorations passionnantes :

-   **Int√©gration CNN :** √âtendre `HybridNet` pour la r√©gression par intervalles sur des images en d√©veloppant des couches de convolution compatibles avec l'arithm√©tique des intervalles.
-   **S√©ries Temporelles :** Combiner le mod√®le avec des **Transformeurs** pour capturer des d√©pendances √† long terme et pr√©dire des "tunnels" de pr√©vision.
-   **Explicabilit√© (XAI) :** Analyser les poids `œÜ` des couches `Macsum` pour interpr√©ter quelles caract√©ristiques apprises sont les plus importantes pour la pr√©diction.
-   **Robustesse et V√©rification :** Utiliser la propagation d'intervalles pour prouver formellement la robustesse du mod√®le face √† des perturbations sur les donn√©es d'entr√©e.